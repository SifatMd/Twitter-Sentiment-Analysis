{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwADBW0hiU-U"
      },
      "source": [
        "# Sentiment Analysis with BERT Sequence Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWGiqv7EhYIF"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JlOL-edoOsa",
        "outputId": "7e091d1a-7646-4a4e-8a48-3849d79e038e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRPdjr9ohav4"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQFOa4bIuYR",
        "outputId": "61367c0b-f398-43ce-f388-27e2ca54df36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.6/dist-packages (0.67.1)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.5.0b1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.18.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: botocore>=1.13.44 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.17.63)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.0.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from streamlit) (20.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.0.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: enum-compat in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.0.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.10.1)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.10.3)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from streamlit) (1.14.63)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (4.1.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.14.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.18.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (4.3.3)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (2.11.2)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.5.1)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.3.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore>=1.13.44->streamlit) (0.15.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore>=1.13.44->streamlit) (0.10.0)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore>=1.13.44->streamlit) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->streamlit) (50.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog->streamlit) (0.1.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit) (0.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit) (0.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit) (1.1.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.7)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (19.0.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.6/dist-packages (4.1.12)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyngrok) (3.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyngrok) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ4X3_zcFLgZ",
        "outputId": "540f2ca2-872a-459a-c201-c88477b3ded8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/fc/18e56e5b1093052bacf6750442410423f3d9785d14ce4f54ab2ac6b112a6/transformers-3.3.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 28.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4721f055ca5001be95a20320a9db0c209643e6bf6b7f529a1d3d2bb62e324948\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDOlhk_ghqjN"
      },
      "source": [
        "## ML Web App with Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSTFp5VheAj"
      },
      "source": [
        "### Write py file for running with Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLVWHHm2iMjW",
        "outputId": "f3639eb8-7277-468b-9eb6-13551f496ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile app.py\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer \n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#!pip install transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader #, RandomSampler, SequentialSampler\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased', \n",
        "    do_lower_case=True \n",
        ")\n",
        "\n",
        "import streamlit as st\n",
        "PAGE_CONFIG = {\"page_title\":\"Sentiment Analysis\",\"page_icon\":\":smiley:\",\"layout\":\"centered\"}\n",
        "st.beta_set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "######################################################################\n",
        "contractions = {\n",
        "\"aight\": \"alright\",\n",
        "\"ain't\": \"am not\",\n",
        "\"amn't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"can not\",\n",
        "\"cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"daren't\": \"dare not\",\n",
        "\"daresn't\": \"dare not\",\n",
        "\"dasn't\": \"dare not\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"d'ye\": \"do you\",\n",
        "\"e'er\": \"ever\",\n",
        "\"everybody's\": \"everybody is\",\n",
        "\"everyone's\": \"everyone is\",\n",
        "\"finna\": \"fixing to\",\n",
        "\"g'day\": \"good day\",\n",
        "\"gimme\": \"give me\",\n",
        "\"giv'n\": \"given\",\n",
        "\"gonna\": \"going to\",\n",
        "\"gon't\": \"go not\",\n",
        "\"gotta\": \"got to\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"had've\": \"had have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'dn't've'd\": \"he would not have had\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"he've\": \"he have\",\n",
        "\"how'd\": \"how would\",\n",
        "\"howdy\": \"how do you do\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how're\": \"how are\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I'm'a\": \"I am about to\",\n",
        "\"I'm'o\": \"I am going to\",\n",
        "\"innit\": \"is it not\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"may've\": \"may have\",\n",
        "\"methinks\": \"me thinks\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"ne'er\": \"never\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"o'er\": \"over\",\n",
        "\"ol'\": \"old\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"'s\": \"is\",\n",
        "\"shalln't\": \"shall not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she shall\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she has\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"somebody's\": \"somebody has\",\n",
        "\"somebody's\": \"somebody is\",\n",
        "\"someone's\": \"someone has\",\n",
        "\"someone's\": \"someone is\",\n",
        "\"something's\": \"something has\",\n",
        "\"something's\": \"something is\",\n",
        "\"so're\": \"so are\",\n",
        "\"that'll\": \"that shall\",\n",
        "\"that'll\": \"that will\",\n",
        "\"that're\": \"that are\",\n",
        "\"that's\": \"that has\",\n",
        "\"that's\": \"that is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd\": \"that had\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'll\": \"there shall\",\n",
        "\"there'll\": \"there will\",\n",
        "\"there're\": \"there are\",\n",
        "\"there's\": \"there has\",\n",
        "\"there's\": \"there is\",\n",
        "\"these're\": \"these are\",\n",
        "\"these've\": \"these have\",\n",
        "\"they'd\": \"they had\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they shall\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they're\": \"they were\",\n",
        "\"they've\": \"they have\",\n",
        "\"this's\": \"this has\",\n",
        "\"this's\": \"this is\",\n",
        "\"those're\": \"those are\",\n",
        "\"those've\": \"those have\",\n",
        "\"'tis\": \"it is\",\n",
        "\"to've\": \"to have\",\n",
        "\"'twas\": \"it was\",\n",
        "\"wanna\": \"want to\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd\": \"we did\",\n",
        "\"we'll\": \"we shall\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'd\": \"what did\",\n",
        "\"what'll\": \"what shall\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what're\": \"what were\",\n",
        "\"what's\": \"what has\",\n",
        "\"what's\": \"what is\",\n",
        "\"what's\": \"what does\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has\",\n",
        "\"when's\": \"when is\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where'll\": \"where shall\",\n",
        "\"where'll\": \"where will\",\n",
        "\"where're\": \"where are\",\n",
        "\"where's\": \"where has\",\n",
        "\"where's\": \"where is\",\n",
        "\"where's\": \"where does\",\n",
        "\"where've\": \"where have\",\n",
        "\"which'd\": \"which had\",\n",
        "\"which'd\": \"which would\",\n",
        "\"which'll\": \"which shall\",\n",
        "\"which'll\": \"which will\",\n",
        "\"which're\": \"which are\",\n",
        "\"which's\": \"which has\",\n",
        "\"which's\": \"which is\",\n",
        "\"which've\": \"which have\",\n",
        "\"who'd\": \"who would\",\n",
        "\"who'd\": \"who had\",\n",
        "\"who'd\": \"who did\",\n",
        "\"who'd've\": \"who would have\",\n",
        "\"who'll\": \"who shall\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who're\": \"who are\",\n",
        "\"who's\": \"who has\",\n",
        "\"who's\": \"who is\",\n",
        "\"who's\": \"who does\",\n",
        "\"who've\": \"who have\",\n",
        "\"why'd\": \"why did\",\n",
        "\"why're\": \"why are\",\n",
        "\"why's\": \"why has\",\n",
        "\"why's\": \"why is\",\n",
        "\"why's\": \"why does\",\n",
        "\"won't\": \"will not\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all'dn't've'd\": \"you all would not have had\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"you'd\": \"you had\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you shall\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\" u \": \"you\",\n",
        "\" ur \": \"your\",\n",
        "\" n \": \"and\"\n",
        "}\n",
        "\n",
        "######################################################################\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\" #last part is used to remove special chars and punctuation\n",
        "\n",
        "def clean_text(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def cont_to_exp(x):\n",
        "    if type(x) is str:\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key,value)\n",
        "        return x\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def preprocessing(text):\n",
        "  text = clean_text(text)\n",
        "\n",
        "  text = cont_to_exp(text)\n",
        "  \n",
        "  text = ' '.join(text.split())\n",
        "\n",
        "  text = BeautifulSoup(text, 'lxml').get_text()\n",
        "\n",
        "  return text \n",
        "\n",
        "\n",
        "def load_model():\n",
        "  from transformers import BertForSequenceClassification\n",
        "  model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels=2,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "  model.to('cuda')\n",
        "  direc_path = \"/content/gdrive/My Drive/Colab Notebooks/Others/Second/BERT Sentiment Analysis/Kaggle 1.6M/Final 1\"\n",
        "  model.load_state_dict(torch.load(os.path.join(direc_path, 'BERT_ft_epoch10.model'), map_location=torch.device('cuda')))\n",
        "  return model \n",
        "\n",
        "\n",
        "def evaluate(dataloader_val, model):\n",
        "  model.eval()\n",
        "\n",
        "  loss_val_total = 0\n",
        "  predictions, true_vals = [], []\n",
        "  for batch in dataloader_val:\n",
        "    batch = tuple(b.to('cuda') for b in batch)\n",
        "\n",
        "    inputs = {\n",
        "        'input_ids':batch[0],\n",
        "        'attention_mask':batch[1],\n",
        "        'labels':batch[2]\n",
        "    }\n",
        "    with torch.no_grad(): #no gradient change here, so no training occurs\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    logits = outputs[1]\n",
        "    loss_val_total += loss.item()\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = inputs['labels'].cpu().numpy()\n",
        "    predictions.append(logits)\n",
        "    true_vals.append(label_ids)\n",
        "  \n",
        "  loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "\n",
        "  predictions = np.concatenate(predictions, axis=0)\n",
        "  true_vals = np.concatenate(true_vals, axis=0)\n",
        "\n",
        "  return loss_val_avg, predictions, true_vals\n",
        "\n",
        "\n",
        "def test_with_input(text, label, model):\n",
        "  text = np.array([text])\n",
        "  label = np.array([label])\n",
        "  encoded_data_test = tokenizer.batch_encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    max_length=256,\n",
        "    padding=True,\n",
        "    return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  input_ids_test = encoded_data_test['input_ids']\n",
        "  attention_masks_test = encoded_data_test['attention_mask']\n",
        "  labels_test = torch.tensor(label)\n",
        "\n",
        "  dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
        "\n",
        "  dataloader_test = DataLoader(\n",
        "    dataset_test\n",
        "  )\n",
        "  _, predictions_test, true_vals_test = evaluate(dataloader_test, model)\n",
        "  return predictions_test\n",
        "\n",
        "\n",
        "def main():\n",
        "  st.title('Streamlit Sentiment Analysis Web App')\n",
        "  menu = ['Home']\n",
        "  st.write(\"\"\" \n",
        "  ## ML Web App using BERT Classifier to detect happy/sad tweets\n",
        "\n",
        "\n",
        "  \"\"\")\n",
        "  st.subheader('Enter Sentence')\n",
        "  user_input = st.text_input('', max_chars=256, value='')\n",
        "\n",
        "  if st.button('Find'):\n",
        "    user_input = preprocessing(user_input)\n",
        "    model = load_model()\n",
        "    label = 1\n",
        "    pred = test_with_input(user_input, label, model)\n",
        "\n",
        "    pred = np.argmax(pred)\n",
        "    emo = ''\n",
        "    if pred == 0:\n",
        "      emo = 'sad'\n",
        "    elif pred == 1:\n",
        "      emo = 'happy'\n",
        "    st.write('Predicted Emotion: {}'.format(emo))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7kHqSvCW36z",
        "outputId": "bb028202-5bfd-4dc6-f9c7-8918d40d6c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "app.py\tgdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhWwMoUyhlcP"
      },
      "source": [
        "### Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV5zJqDEkDYj",
        "outputId": "af12679e-d357-4135-98be-1d421b54d5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ngrok authtoken xxxxxxxxxxx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtYRXskdhzNa"
      },
      "source": [
        "### Run file with streamlit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKR3VAn2W4Zu"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY1l6YVFh9mk"
      },
      "source": [
        "### Create public url with ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEq9E_TKW4sQ",
        "outputId": "6b60aa3a-449e-4de6-94c2-3e181ae8a6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://e23a7c0bd778.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuF0knMtaWze"
      },
      "source": [
        "## Kill Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGwyiK1KadE7"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2MyvlQJhPhY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}